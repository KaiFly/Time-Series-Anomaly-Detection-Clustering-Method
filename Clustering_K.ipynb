{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tqdm.notebook as tq\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import utils_cl\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meter_id</th>\n",
       "      <th>START_WEEKDAY</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>CN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PP01000100340001</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>0.321275</td>\n",
       "      <td>0.040023</td>\n",
       "      <td>-0.185360</td>\n",
       "      <td>-0.097297</td>\n",
       "      <td>0.104045</td>\n",
       "      <td>-0.170828</td>\n",
       "      <td>-0.082016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PP01000100340001</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>0.032041</td>\n",
       "      <td>0.195984</td>\n",
       "      <td>-0.085221</td>\n",
       "      <td>-0.029924</td>\n",
       "      <td>-0.038465</td>\n",
       "      <td>0.257508</td>\n",
       "      <td>-0.087102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PP01000100340001</td>\n",
       "      <td>2017-01-16</td>\n",
       "      <td>-0.121479</td>\n",
       "      <td>-0.100963</td>\n",
       "      <td>0.163844</td>\n",
       "      <td>-0.019069</td>\n",
       "      <td>-0.015114</td>\n",
       "      <td>-0.049732</td>\n",
       "      <td>0.037160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           meter_id START_WEEKDAY        T2        T3        T4        T5  \\\n",
       "0  PP01000100340001    2017-01-02  0.321275  0.040023 -0.185360 -0.097297   \n",
       "1  PP01000100340001    2017-01-09  0.032041  0.195984 -0.085221 -0.029924   \n",
       "2  PP01000100340001    2017-01-16 -0.121479 -0.100963  0.163844 -0.019069   \n",
       "\n",
       "         T6        T7        CN  \n",
       "0  0.104045 -0.170828 -0.082016  \n",
       "1 -0.038465  0.257508 -0.087102  \n",
       "2 -0.015114 -0.049732  0.037160  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = pd.read_csv(\"dataset/residual_weekly_consumption.csv\")\n",
    "full_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_fraud = ['PP03000806717001',\n",
    " 'PP01000624985001',\n",
    " 'PP09000811262001',\n",
    " 'PP07000600209001',\n",
    " 'PP05000515674001',\n",
    " 'PP07000613618001',\n",
    " 'PP05000944240001',\n",
    " 'PP07000683750001',\n",
    " 'PP05000501075001',\n",
    " 'PP01000101975001',\n",
    " 'PP05000504291001',\n",
    " 'PP05000517007001',\n",
    " 'PP05000524362001',\n",
    " 'PP09000907245001',\n",
    " 'PP01000103454001',\n",
    " 'PP05000515078001',\n",
    " 'PP01000621525001',\n",
    " 'PP07000668984001',\n",
    " 'PP03000804137001',\n",
    " 'PP09000802199001',\n",
    " 'PP03000809863001',\n",
    " 'PP05000960339001',\n",
    " 'PP03000810040001',\n",
    " 'PP03000804097001',\n",
    " 'PP07000693227001',\n",
    " 'PP05000524904001',\n",
    " 'PP05000951373001',\n",
    " 'PP05000989390001',\n",
    " 'PP05000961578001',\n",
    " 'PP07000613734001',\n",
    " 'PP09000892588001',\n",
    " 'PP01000101145001',\n",
    " 'PP01000117448001',\n",
    " 'PP09000810551001',\n",
    " 'PP09000837585001',\n",
    " 'PP05000968892001',\n",
    " 'PP09000890287001',\n",
    " 'PP09000808816001',\n",
    " 'PP03000832565001',\n",
    " 'PP09000821975001',\n",
    " 'PP05000509756001',\n",
    " 'PP05000967753001',\n",
    " 'PP05000941329001',\n",
    " 'PP05000953931001',\n",
    " 'PP09000817228001',\n",
    " 'PP03000898466001',\n",
    " 'PP05000522325001',\n",
    " 'PP07000600208001',\n",
    " 'PP05000948327001',\n",
    " 'PP01000140094001',\n",
    " 'PP05000501411001',\n",
    " 'PP07000673219001',\n",
    " 'PP07000682085001',\n",
    " 'PP01000131431001',\n",
    " 'PP07000700839001',\n",
    " 'PP07000713473001',\n",
    " 'PP05000977900001',\n",
    " 'PP07000678169001',\n",
    " 'PP03000805922001',\n",
    " 'PP09000808752001',\n",
    " 'PP09000120555001',\n",
    " 'PP05000502843001',\n",
    " 'PP05000516894001',\n",
    " 'PP07000683751001',\n",
    " 'PP07000684209001',\n",
    " 'PP05000967512001']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X_principal = pca.fit_transform(full_df.iloc[:, 2:])\n",
    "X_principal = pddd.DataFrame(X_principal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_principal = full_df.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scheme_used():\n",
    "    \n",
    "    X_normalized = X_principal\n",
    "    df2 = full_df\n",
    "    \n",
    "    from sklearn.cluster import KMeans\n",
    "    distance_used = utils_cl.distance_ED\n",
    "    cluster_number  = 3\n",
    "    n_iters = 750\n",
    "    km = KMeans(\n",
    "        n_clusters=cluster_number, init='random',\n",
    "        n_init=10, max_iter=n_iters,\n",
    "        tol=1e-04, random_state=200\n",
    "    )\n",
    "\n",
    "    y_km = km.fit_predict(X_normalized)\n",
    "    centroids = km.cluster_centers_\n",
    "    \n",
    "    dict_DC = {}\n",
    "    for i in range(cluster_number):\n",
    "        dict_DC[i+1] = []\n",
    "    for i, j in tq.tqdm_notebook(enumerate(y_km), total = len(y_km)):        \n",
    "        dict_DC[j+1].append(distance_used(centroids[j], X_normalized.iloc[i, :])) \n",
    "    quantile_radius = []\n",
    "    percentile = 0.95\n",
    "    for j in range(cluster_number):\n",
    "        quantile_radius.append(pd.DataFrame(dict_DC[j+1]).quantile(percentile, axis = 0)[0])\n",
    "    def test_fraud_cluster(X_sample, cluster_belong, radius_cent, thresh_KM):\n",
    "        dist2cent = distance_used(X_sample, centroids[cluster_belong - 1])\n",
    "        is_fraud = (dist2cent > radius_cent[cluster_belong-1] * thresh_KM)\n",
    "        return is_fraud\n",
    "    def prediction_result(test_set, radius_cent, thresh_KM = 1.0):\n",
    "        N = test_set.shape[0]\n",
    "        mdd_fraud_1week = []\n",
    "        Y_predict = []\n",
    "        ind_fraud_predicted = []\n",
    "        count_fraud_predicted = [0]*cluster_number\n",
    "        for i in tq.tqdm_notebook(range(N)):\n",
    "            X_sample = test_set.iloc[i, :]\n",
    "            cluster_belong = y_km[i] + 1\n",
    "            test_result = test_fraud_cluster(X_sample, cluster_belong, radius_cent, thresh_KM)\n",
    "            if test_result == True :\n",
    "                count_fraud_predicted[cluster_belong-1] += 1\n",
    "                mdd_fraud_1week.append(df2.iloc[i, 0])\n",
    "                ind_fraud_predicted.append(i)\n",
    "            Y_predict.append(test_result)\n",
    "        return list(set(mdd_fraud_1week)), Y_predict, ind_fraud_predicted,count_fraud_predicted\n",
    "    \n",
    "    test_set = X_normalized\n",
    "    radius_cent = quantile_radius\n",
    "    thresh_KM = 1\n",
    "    mdd_fraud_1week, Y_predict, ind_fraud_predicted, count_fraud_predicted = prediction_result(test_set, radius_cent, thresh_KM)\n",
    "    test_df = pd.DataFrame()\n",
    "    test_df['meter_id'] = df2.meter_id\n",
    "    test_df['PRED'] = Y_predict\n",
    "    test_df = test_df.groupby(['meter_id'])['PRED'].apply(list)\n",
    "    \n",
    "    consecutive_week = 2\n",
    "    def test_consecutive_week(l):\n",
    "        n = len(l)\n",
    "        for i in range(n) :\n",
    "            if l[i] == True:\n",
    "                if sum(l[i : i+consecutive_week]) == consecutive_week:\n",
    "                    return True\n",
    "            else :\n",
    "                continue\n",
    "        return False\n",
    "    mdd_fraud_n_week = []\n",
    "    for i in range(test_df.shape[0]):\n",
    "        l = test_df[i]\n",
    "        if test_consecutive_week(l):\n",
    "            mdd_fraud_n_week.append(test_df.index[i])\n",
    "            \n",
    "    n_fraud = len(set(mdd_fraud_n_week) & set(list_fraud))\n",
    "    res_1 = n_fraud/len(list_fraud)\n",
    "    res_2 = (len(mdd_fraud_n_week) - n_fraud)/1000\n",
    "    return res_1, res_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4bfcb1039674aa298e695811d04f485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=66540.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6629872e9cb4d418351026986ceb3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=66540.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.45454545454545453, 0.202)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Scheme2():\n",
    "    X_normalized = full_df.iloc[:, 2:]\n",
    "    df2 = full_df\n",
    "    \n",
    "    from sklearn.cluster import KMeans\n",
    "    distance_used = utils_cl.distance_ED\n",
    "    cluster_number  = 3\n",
    "    n_iters = 750\n",
    "    km = KMeans(\n",
    "        n_clusters=cluster_number, init='random',\n",
    "        n_init=10, max_iter=n_iters,\n",
    "        tol=1e-04, random_state=200\n",
    "    )\n",
    "\n",
    "    y_km = km.fit_predict(X_normalized)\n",
    "    centroids = km.cluster_centers_\n",
    "    \n",
    "    dict_DC = {}\n",
    "    for i in range(cluster_number):\n",
    "        dict_DC[i+1] = []\n",
    "    for i, j in tq.tqdm_notebook(enumerate(y_km), total = len(y_km)):        \n",
    "        dict_DC[j+1].append(distance_used(centroids[j], X_normalized.iloc[i, :])) \n",
    "    quantile_radius = []\n",
    "    percentile = 0.95\n",
    "    for j in range(cluster_number):\n",
    "        quantile_radius.append(pd.DataFrame(dict_DC[j+1]).quantile(percentile, axis = 0)[0])\n",
    "    def test_fraud_cluster(X_sample, cluster_belong, radius_cent, thresh_KM):\n",
    "        dist2cent = distance_used(X_sample, centroids[cluster_belong - 1])\n",
    "        is_fraud = (dist2cent > radius_cent[cluster_belong-1] * thresh_KM)\n",
    "        return is_fraud\n",
    "    def prediction_result(test_set, radius_cent, thresh_KM = 1.0):\n",
    "        N = test_set.shape[0]\n",
    "        mdd_fraud_1week = []\n",
    "        Y_predict = []\n",
    "        ind_fraud_predicted = []\n",
    "        count_fraud_predicted = [0]*cluster_number\n",
    "        for i in tq.tqdm_notebook(range(N)):\n",
    "            X_sample = test_set.iloc[i, :]\n",
    "            cluster_belong = y_km[i] + 1\n",
    "            test_result = test_fraud_cluster(X_sample, cluster_belong, radius_cent, thresh_KM)\n",
    "            if test_result == True :\n",
    "                count_fraud_predicted[cluster_belong-1] += 1\n",
    "                mdd_fraud_1week.append(df2.iloc[i, 0])\n",
    "                ind_fraud_predicted.append(i)\n",
    "            Y_predict.append(test_result)\n",
    "        return list(set(mdd_fraud_1week)), Y_predict, ind_fraud_predicted,count_fraud_predicted\n",
    "    \n",
    "    test_set = X_normalized\n",
    "    radius_cent = quantile_radius\n",
    "    thresh_KM = 1\n",
    "    mdd_fraud_1week, Y_predict, ind_fraud_predicted, count_fraud_predicted = prediction_result(test_set, radius_cent, thresh_KM)\n",
    "    test_df = pd.DataFrame()\n",
    "    test_df['meter_id'] = df2.meter_id\n",
    "    test_df['PRED'] = Y_predict\n",
    "    test_df = test_df.groupby(['meter_id'])['PRED'].apply(list)\n",
    "    \n",
    "    consecutive_week = 2\n",
    "    def test_consecutive_week(l):\n",
    "        n = len(l)\n",
    "        for i in range(n) :\n",
    "            if l[i] == True:\n",
    "                if sum(l[i : i+consecutive_week]) == consecutive_week:\n",
    "                    return True\n",
    "            else :\n",
    "                continue\n",
    "        return False\n",
    "    mdd_fraud_n_week = []\n",
    "    for i in range(test_df.shape[0]):\n",
    "        l = test_df[i]\n",
    "        if test_consecutive_week(l):\n",
    "            mdd_fraud_n_week.append(test_df.index[i])\n",
    "            \n",
    "    n_fraud = len(set(mdd_fraud_n_week) & set(list_fraud))\n",
    "    res_1 = n_fraud/len(list_fraud)\n",
    "    res_2 = (len(mdd_fraud_n_week) - n_fraud)/1000\n",
    "    return res_1, res_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "X_normalized = full_df.iloc[:, 2:]\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X_principal = pca.fit_transform(full_df.iloc[:, 2:])\n",
    "X_principal = pd.DataFrame(X_principal)\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "# DBSCAN\n",
    "eps = 0.2\n",
    "min_samples = 10\n",
    "labels_1 = utils_cl._DBSCAN(X_principal, eps, min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66540, 7)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(labels_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
